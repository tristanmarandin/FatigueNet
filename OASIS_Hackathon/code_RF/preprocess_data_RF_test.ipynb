{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99483d66",
   "metadata": {},
   "source": [
    "### I- Importations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3cd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a3eb5",
   "metadata": {},
   "source": [
    "### II- Import our CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d064e65",
   "metadata": {},
   "source": [
    "We give the directory where all our data is stockage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcb182e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"../data/dataset_9/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d251a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_anthropo = pd.read_csv(DIR+\"data_anthropo.csv\")\n",
    "meteo_data = pd.read_csv(DIR+\"meteo_data.csv\")\n",
    "player_info = pd.read_csv(DIR+\"player_info.csv\")\n",
    "catapult_dir =  DIR+\"test_data/catapult_data/\"\n",
    "RPE_TEST_FINAL = pd.read_csv(DIR+\"test_data/RPE_TEST_FINAL.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b9895",
   "metadata": {},
   "source": [
    "### III- Work on Data_anthropo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83196ff8",
   "metadata": {},
   "source": [
    "We value of the column date to datetime objects and after that we extract only the date component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70791fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_anthropo[\"n_date\"] = pd.to_datetime(data_anthropo[\"n_date\"]).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188c89e",
   "metadata": {},
   "source": [
    "### IV- Work on Meteo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d6dd9",
   "metadata": {},
   "source": [
    "We create a new function that differentiates between morning and afternoon depending on the time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d1f053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_day(h):\n",
    "    if h>=datetime.strptime(\"12:00\",'%H:%M') and h<datetime.strptime(\"23:00\",'%H:%M') :\n",
    "        return \"PM\"\n",
    "    \n",
    "    elif h>=datetime.strptime(\"08:00\",'%H:%M') and h<datetime.strptime(\"12:00\",'%H:%M'):\n",
    "        return \"AM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "043e1bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_meteo(meteo_data):\n",
    "    '''\n",
    "    Function to add a column with AM and PM and drop column Heure and is.day().\n",
    "    '''\n",
    "    meteo_data['Heure'] = pd.to_datetime(meteo_data['Heure'], format='%H:%M')\n",
    "    \n",
    "    meteo_data['AM/PM'] = meteo_data['Heure'].apply(half_day)\n",
    "    \n",
    "    meteo_data_processed = meteo_data.drop(['Heure', 'is_day ()'], axis=1)\n",
    "    \n",
    "    meteo_data_processed = meteo_data_processed.groupby(['time', 'AM/PM']).mean()\n",
    "    \n",
    "    return meteo_data_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88022b74",
   "metadata": {},
   "source": [
    "### V- Work on Catapult_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b4ce249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_csv_files(folder_path):\n",
    "    '''\n",
    "    Function to load all Catapult files and concatenate them.\n",
    "    '''    \n",
    "    # We create a list to stock dataframes of each CSV\n",
    "    dfs = []\n",
    "\n",
    "    # We go throught all files of the folder.\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # We read the csv and we add the dataframe to the list.\n",
    "            df = pd.read_csv(file_path)\n",
    "            dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single one\n",
    "    result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae6e8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heart_rate(df):\n",
    "    \n",
    "    # Group by 'catapult_id' and calculate the maximum 'Heart rate' value for each group\n",
    "    max_heart_rate = df.groupby('catapult_id')['Heart rate'].max()\n",
    "\n",
    "    # Create a new 'Heart_rate_Max' column in the initial DataFrame\n",
    "    df['Heart_rate_Max'] = df['catapult_id'].map(max_heart_rate)\n",
    "    \n",
    "    # Calculate the ratio of 'Heart rate' to 'Heart_rate_Max' and create a new 'Heart_rate_Ratio' column\n",
    "    df['Heart_rate_Ratio'] = df['Heart rate'] / df['Heart_rate_Max']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e5bc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def montpellier_or_not(df):\n",
    "    \"\"\"\n",
    "    This function separates training sessions (and matches) that took place in Montpellier from those that did not.\n",
    "    \"\"\"\n",
    "    # Create conditions to filter the DataFrame\n",
    "    \n",
    "    conditions = (\n",
    "        (df['Latitude'] >= 43.37) & (df['Latitude'] <= 43.75) &\n",
    "        (df['Longitude'] >= 3.52) & (df['Longitude'] <= 4.25)\n",
    "    )\n",
    "\n",
    "    # Filter the DataFrame and create a new DataFrame with the deleted rows\n",
    "    montpellier_df = df[conditions]\n",
    "\n",
    "    return montpellier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a04bbbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainning(data):\n",
    "    \"\"\"\n",
    "    Function which formats time columns.\n",
    "    \"\"\"\n",
    "    # Convert 'hour-minutes-second' column to 'H:M:S' format\n",
    "    data['hour-minutes-second'] = data['hour-minutes-second'].astype(str).str.replace('-', ':')\n",
    "\n",
    "    # Convert the 'hour-minutes-second' column into datetime format with separate hours, minutes and seconds\n",
    "    data['hour-minutes-second'] = pd.to_datetime(data['hour-minutes-second'], format='%H:%M:%S', errors='coerce')\n",
    "\n",
    "    # Ensure that the column is of type datetime64[ns].\n",
    "    data['hour-minutes-second'] = pd.to_datetime(data['hour-minutes-second'], errors='coerce')\n",
    "\n",
    "    data['Hour'] = data['hour-minutes-second'].dt.hour\n",
    "    \n",
    "    # Add column 'AM/PM'\n",
    "    data['am-pm'] = 'AM' \n",
    "    data.loc[data['Hour'] >= 13, 'am-pm'] = 'PM'\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69adc98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_heart_rate_values(df):\n",
    "    # Créer une série pour stocker les valeurs remplacées\n",
    "    filled = df[\"Heart rate\"].copy()\n",
    "\n",
    "    # Identifier les cellules à remplir\n",
    "    #mask = filled < 2.0\n",
    "    mask = filled.reset_index(drop=True) < 2.0\n",
    "\n",
    "    # Trouver les valeurs suivantes et précédentes valides\n",
    "    next_valid = df.groupby('catapult_id')[\"Heart rate\"].apply(lambda x: x.shift(-1).ffill())\n",
    "    prev_valid = df.groupby('catapult_id')[\"Heart rate\"].apply(lambda x: x.shift(1).bfill())\n",
    "\n",
    "    # Choisir la valeur suivante ou précédente en fonction de la condition\n",
    "    filled[mask] = next_valid[mask].where(next_valid[mask] > 2.0, prev_valid[mask])\n",
    "\n",
    "    # Assurer que les cellules sans valeurs valides soient mises à 0\n",
    "    filled.fillna(0.0, inplace=True)\n",
    "    \n",
    "    df['Heart rate'] = filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f39be4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catapult_preprocess(folder_path):\n",
    "    \n",
    "    # Concatenate all catapults files  \n",
    "    df = concat_csv_files(folder_path)\n",
    "    \n",
    "    # Drop column centisecond, x and y \n",
    "    df.drop(['Centiseconds','x','y'], axis=1, inplace=True)\n",
    "    \n",
    "    # Fill columns empty\n",
    "    #fill_missing_heart_rate_values(df)\n",
    "\n",
    "    # Create a new feature of Heart ratio, which will be more revelant \n",
    "    df_heart = heart_rate(df)\n",
    "    \n",
    "    # Drop column Heart rate Max, Heart rate, Latitude and Longitude\n",
    "    df_heart.drop(['Heart_rate_Max', 'Heart rate', 'Latitude', 'Longitude'], axis=1, inplace=True)\n",
    "    \n",
    "    # Create a new feature of AM/PM to differentiate trainning of morning and evening\n",
    "    df_train = create_trainning(df_heart)\n",
    "    \n",
    "    # Supprimer la colonne originale 'hour-minutes-second'\n",
    "    df_train.drop(['Hour','hour-minutes-second'], axis=1, inplace=True)\n",
    "\n",
    "    return df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6583c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../data/dataset_9/test_data/catapult_data/'\n",
    "\n",
    "df = catapult_preprocess(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9faae6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(['n_date', 'am-pm', 'catapult_id']).agg({\n",
    "    'Odometer': 'last',\n",
    "    'Velocity': 'mean',\n",
    "    'Acceleration': 'mean',\n",
    "    'PlayerLoad': 'last',\n",
    "    'Metabolic power': 'mean',\n",
    "    'Smooth Load': 'mean',\n",
    "    'Heart_rate_Ratio': 'mean'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d96158f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_catapult_processed = pd.merge(df, player_info, on='catapult_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d97450f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RPE_TEST_FINAL[\"n_date\"] = pd.to_datetime(RPE_TEST_FINAL[\"n_date\"]).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2fd4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "RPE_TEST_FINAL[\"duration\"].fillna(RPE_TEST_FINAL['duration'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faebad05",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4075dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RPE_TEST_FINAL['n_date'] = pd.to_datetime(RPE_TEST_FINAL[\"n_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaf453cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meteo_data_processed = preprocess_meteo(meteo_data)\n",
    "meteo_data_processed = meteo_data_processed.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "526b7bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_data_processed['time'] = pd.to_datetime(meteo_data_processed['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e43a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_global = pd.merge(RPE_TEST_FINAL, meteo_data_processed, left_on=[\"n_date\",\"am-pm\"], right_on=[\"time\",\"AM/PM\"], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57fbbdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_global = data_global.drop([\"time\", \"AM/PM\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc72638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_global['n_date'] = pd.to_datetime(data_global['n_date'])\n",
    "data_anthropo['n_date'] = pd.to_datetime(data_anthropo['n_date'])\n",
    "data_anthropo = data_anthropo.sort_values(['n_date', 'player_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b7e9add",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_global = pd.merge_asof(data_global, data_anthropo, on='n_date', by='player_id', direction='forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c62038f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_catapult_processed['n_date'] = pd.to_datetime(data_catapult_processed['n_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0104fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_global = pd.merge(data_global, data_catapult_processed, on=['n_date', 'am-pm', 'player_id'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cdcec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data_global.drop([\"hour\", \"IS_TEST\", 'team', 'catapult_id', 'am-pm', 'position_code', 'full_position_code'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29b04e4",
   "metadata": {},
   "source": [
    "### Previous RPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d57e4051",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_previous_rpe(rpe_data,n_previous):\n",
    "    # Sort the DataFrame by 'player_id' and 'n_date' to ensure correct ordering\n",
    "    rpe_data.sort_values(by=['player_id', 'n_date'], inplace=True)\n",
    "\n",
    "    # Create new columns for n_previous previous RPE values\n",
    "    for i in range(1, n_previous + 1):\n",
    "        col_name = f'previous_RPE_{i}'\n",
    "        rpe_data[col_name] = rpe_data.groupby('player_id')['RPE'].shift(i)\n",
    "        rpe_data[col_name] = rpe_data.groupby('player_id')[col_name].fillna(method='ffill')\n",
    "\n",
    "    return rpe_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0b873d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.fillna(0, inplace=True)\n",
    "data_final.replace('#DIV/0!', 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "588f9ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécifiez le chemin du fichier CSV où vous souhaitez sauvegarder le DataFrame\n",
    "output_csv_path = \"/home/docker/code_RF/dataset_rf_test\"\n",
    "# Utilisez la méthode to_csv pour sauvegarder le DataFrame dans un fichier CSV\n",
    "data_final.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68213e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22980/3101493199.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  rpe_data[col_name] = rpe_data.groupby('player_id')[col_name].fillna(method='ffill')\n",
      "/tmp/ipykernel_22980/3101493199.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  rpe_data[col_name] = rpe_data.groupby('player_id')[col_name].fillna(method='ffill')\n",
      "/tmp/ipykernel_22980/3101493199.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  rpe_data[col_name] = rpe_data.groupby('player_id')[col_name].fillna(method='ffill')\n",
      "/tmp/ipykernel_22980/3101493199.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  rpe_data[col_name] = rpe_data.groupby('player_id')[col_name].fillna(method='ffill')\n",
      "/tmp/ipykernel_22980/3101493199.py:9: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  rpe_data[col_name] = rpe_data.groupby('player_id')[col_name].fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "add_previous_rpe(data_final,5)\n",
    "# Spécifiez le chemin du fichier CSV où vous souhaitez sauvegarder le DataFrame\n",
    "output_csv_path = \"/home/docker/code_RF/dataset_rf_test_with_5_previous\"\n",
    "# Utilisez la méthode to_csv pour sauvegarder le DataFrame dans un fichier CSV\n",
    "data_final.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace4e078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
