{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FATIGUE NET**\n",
    "\n",
    "- Imports\n",
    "- Definition of custom dataset\n",
    "- Definition of our FatigueNet model class\n",
    "- Training and testing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatapultDataset(Dataset):\n",
    "    def __init__(self, dataset_list):\n",
    "        self.catapult = [data[0] for data in dataset_list]\n",
    "        self.targets = [data[1][-1] for data in dataset_list]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.catapult)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sample = torch.Tensor(self.catapult[idx].values.astype(np.float64))\n",
    "        target_sample = torch.Tensor([self.targets[idx]])\n",
    "        \n",
    "        return input_sample, target_sample\n",
    "    \n",
    "class JMDataset(Dataset):\n",
    "    def __init__(self, dataset_list):\n",
    "        self.targets = [data[1].pop() for data in dataset_list]\n",
    "        self.jm = [data[1] for data in dataset_list]\n",
    "        self.jm = np.array(self.jm)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.jm)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sample = torch.Tensor(self.jm[idx].astype(np.float64))\n",
    "        target_sample = torch.Tensor([self.targets[idx]])\n",
    "        \n",
    "        return input_sample, target_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FatigueNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FatigueNet(nn.Module):\n",
    "    def __init__(self, nb_features_catapult, nb_features_jm):\n",
    "        super(FatigueNet, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=nb_features_catapult, out_channels=16, kernel_size=20, stride=1, padding=9),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=16, out_channels=16, kernel_size=20, stride=1, padding=9),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.AvgPool1d(kernel_size=4, stride=4, padding=2),\n",
    "            nn.Conv1d(16, 32, kernel_size=15, stride=1, padding=7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, kernel_size=15, stride=1, padding=7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.AvgPool1d(kernel_size=4, stride=4, padding=2),\n",
    "            nn.Conv1d(32, 64, kernel_size=10, stride=1, padding=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=10, stride=1, padding=4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.AvgPool1d(kernel_size=4, stride=4, padding=2),\n",
    "            nn.Conv1d(64, 128, kernel_size=10, stride=1, padding=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, kernel_size=10, stride=1, padding=4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv1d(128, 256, kernel_size=10, stride=1, padding=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, kernel_size=10, stride=1, padding=4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv1d(256, 512, kernel_size=10, stride=1, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(512, 512, kernel_size=10, stride=1, padding=4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.block7 = nn.Sequential(\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2, padding=1),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "        \n",
    "        self.gru = nn.GRU(512, 256, num_layers=3, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(256, 1)\n",
    "        self.block8 = nn.Sequential(\n",
    "            nn.Linear(256 + nb_features_jm, 256 + nb_features_jm),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256 + nb_features_jm, 256 + nb_features_jm),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256 + nb_features_jm, 256 + nb_features_jm),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256 + nb_features_jm, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.permute(0, 2, 1)\n",
    "        pool = nn.AvgPool1d(kernel_size=50, stride=int(x1.shape[2]/4000), padding=25)\n",
    "\n",
    "        x1 = pool(x1)\n",
    "        x1 = self.block1(x1)\n",
    "        x1 = self.block2(x1)\n",
    "        x1 = self.block3(x1)\n",
    "        x1 = self.block4(x1)\n",
    "        x1 = self.block5(x1)\n",
    "        x1 = self.block6(x1)\n",
    "        x1 = self.block7(x1)\n",
    "        \n",
    "        # Apply GRU layers\n",
    "        x1 = x1.permute(0, 2, 1)\n",
    "        x1, _ = self.gru(x1)\n",
    "        \n",
    "        # Select the last sequence of each batch (assuming batch_first=True)\n",
    "        x1 = x1[:, -1, :]\n",
    "        \n",
    "        # Apply the fully connected layer\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.block8(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def train_with_gradient_accumulation(self, train_loader, train_jm_loader, num_epochs, accumulation_steps, device, show_log=True):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        criterion = nn.L1Loss()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            self.to(device)\n",
    "            self.train()  # Met le modèle en mode entraînement\n",
    "\n",
    "            assert len(train_jm_loader) == len(train_loader)\n",
    "\n",
    "            combined_loader = zip(train_jm_loader, train_loader)\n",
    "            i = 0\n",
    "            for (inputs_jm, targets_jm), (inputs, targets) in combined_loader:\n",
    "\n",
    "                inputs_jm = inputs_jm.to(device)\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self(inputs, inputs_jm)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                loss = loss / accumulation_steps  # Normalisation de la perte\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient accumulation\n",
    "                if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                i += 1\n",
    "\n",
    "            if show_log : print(f'Epoch {epoch + 1}, MAE: {running_loss / len(train_loader)}')\n",
    "\n",
    "        if show_log : print('Training Done')\n",
    "\n",
    "    def predict(self, test_loader, test_jm_loader, device):\n",
    "        self.eval()\n",
    "        mae = 0.0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            assert len(test_jm_loader) == len(test_loader)\n",
    "\n",
    "            combined_loader = zip(test_jm_loader, test_loader)\n",
    "            \n",
    "            for (inputs_jm, targets_jm), (inputs, targets) in combined_loader:\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                inputs_jm = inputs_jm.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self(inputs, inputs_jm)\n",
    "\n",
    "                # Calcul de la MAE pour ce lot\n",
    "                batch_mae = mae = torch.abs(outputs - targets).mean()\n",
    "                mae += batch_mae.item()\n",
    "\n",
    "                predictions = outputs.cpu().numpy()\n",
    "                targets = targets.cpu().numpy()\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_targets.extend(targets)\n",
    "\n",
    "        return all_predictions, all_targets\n",
    "\n",
    "    def evaluate_model(self, test_loader, test_jm_loader, device, show_cm=False):\n",
    "        all_predictions, all_targets = self.predict_model(self, test_loader, test_jm_loader, device)\n",
    "\n",
    "        # Average MAE\n",
    "        mae /= len(test_loader)\n",
    "        num_classes = 10\n",
    "        \n",
    "        predictions_discretized = [int(round(value[0])) for value in all_predictions]\n",
    "        targets_discretized = [int(y) for y in all_targets]\n",
    "        \n",
    "        if show_cm:\n",
    "            cm = confusion_matrix(targets_discretized, predictions_discretized, labels=range(1, num_classes + 1))\n",
    "\n",
    "            sns.set(font_scale=1.2)\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, square=True)\n",
    "            plt.xlabel('Predicted Class')\n",
    "            plt.ylabel('Actual Class')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.show()\n",
    "\n",
    "        return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_catapult_jm_loader(train_catapult_path, train_jm_path, player_id, batch_size, gnr, is_train=True):\n",
    "    # Get all catapult data per session for the player_id\n",
    "    catapult_data = pd.read_csv(f\"{train_catapult_path}{player_id}.csv\")\n",
    "    catapult_by_sessions = catapult_data.groupby(['n_date', 'session_id'])\n",
    "    catapult_by_sessions_dict = {}\n",
    "    for session, df in catapult_by_sessions:\n",
    "        catapult_by_sessions_dict[session] = df\n",
    "        if is_train : catapult_by_sessions_dict[session].drop(\"Unnamed: 0\", axis=1, inplace = True)\n",
    "    \n",
    "    # Get all JM data per session for the player_id\n",
    "    jm = pd.read_csv(f\"{train_jm_path}{player_id}.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "    print(len(jm.columns) - 3)\n",
    "\n",
    "    dataset = []\n",
    "    # Create a tuple list containing catapult and JM data per session\n",
    "    for session_jm in jm.values.tolist():\n",
    "        try:\n",
    "            dataset.append( [catapult_by_sessions_dict[(session_jm[-2], session_jm[-1])], session_jm[:-2]] )\n",
    "        except:\n",
    "            print(\"Session not found:\", (session_jm[-2], session_jm[-1]))\n",
    "\n",
    "    # Dropt n_date and session_id now that we have used them\n",
    "    for data in dataset:\n",
    "        data[0].drop(['n_date', 'session_id'], axis=1, inplace=True)\n",
    "\n",
    "    # Create custom datasets\n",
    "    catapult_dataset = CatapultDataset(dataset)\n",
    "    jm_dataset = JMDataset(dataset)\n",
    "\n",
    "    # Create dataset loaders\n",
    "    train_catapult_loader = DataLoader(catapult_dataset, batch_size=batch_size, shuffle=False, generator=gnr)\n",
    "    train_jm_loader = DataLoader(jm_dataset, batch_size=batch_size, shuffle=False, generator=gnr)\n",
    "    return train_catapult_loader, train_jm_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyper-parameters\n",
    "device = \"cuda\"\n",
    "num_epochs = 10\n",
    "batch_size = 4\n",
    "accumulation_steps = 4\n",
    "nb_features_catapult = 6\n",
    "nb_features_jm = 20\n",
    "\n",
    "# Seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "gnr = torch.Generator().manual_seed(seed)\n",
    "\n",
    "# Dictionary to store model and mae for each player\n",
    "models = {}\n",
    "mae_dict = {}\n",
    "prediction_by_player = {}\n",
    "\n",
    "# File path definitions\n",
    "train_catapult_path = \"data/catapult_by_player_final/\"\n",
    "train_jm_path = \"data/jm_clean_by_player/\"\n",
    "test_catapult_path = \"data/test/catapult_by_player_normalized/\"\n",
    "test_jm_path = \"data/test/jm_by_player/\"\n",
    "\n",
    "# Get list of test files\n",
    "train_jm_files = os.listdir(train_jm_path)\n",
    "test_jm_files = os.listdir(test_jm_path)\n",
    "\n",
    "# Train and test for each player -------------------------------------------- #\n",
    "for player_id in train_jm_files:\n",
    "    player_id = player_id.split(\".\")[0]\n",
    "\n",
    "    if player_id not in test_jm_files:\n",
    "        print(f\"{player_id} not in test\")\n",
    "        continue\n",
    "    \n",
    "    # Create dataset loaders\n",
    "    train_catapult_loader, train_jm_loader = create_catapult_jm_loader(train_catapult_path, train_jm_path, player_id, batch_size, gnr)\n",
    "    test_catapult_loader, test_jm_loader = create_catapult_jm_loader(test_catapult_path, test_jm_path + player_id + \"/\", player_id, batch_size, gnr, is_train=False)\n",
    "\n",
    "    # Training\n",
    "    model = FatigueNet(nb_features_catapult, nb_features_jm)\n",
    "    model.train_with_gradient_accumulation(train_catapult_loader, train_jm_loader, num_epochs, accumulation_steps, device)\n",
    "    models[player_id] = model\n",
    "\n",
    "    # Testing\n",
    "    test_mae = model.evaluate_model(test_catapult_loader, test_jm_loader, device)\n",
    "    mae_dict[player_id] = test_mae\n",
    "    print(f'MAE on test dataset : {test_mae}')\n",
    "\n",
    "# Average MAE  ------------------------------------------------------------- #\n",
    "values = list(mae_dict.values())\n",
    "average = sum(values) / len(values)\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE for 1 model 1 player : 1.7521\n",
    "\n",
    "MAE for 1 global model : 3.1936"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OASIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
