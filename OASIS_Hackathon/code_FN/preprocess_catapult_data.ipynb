{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116ae9a6",
   "metadata": {},
   "source": [
    "# **PREPROCESS CATAPULT DATA**\n",
    "\n",
    "- Adding session id to raw catapult data\n",
    "- Process catapult data\n",
    "    1. Concatenating all data per player\n",
    "    2. Cleaning Hear rate feature\n",
    "    3. Split concatenated dataset by player\n",
    "    4. Split each player dataset by session\n",
    "    5. Get max session length for each player\n",
    "    6. Pad and normalize session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e132b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05794f4",
   "metadata": {},
   "source": [
    "# Adding session id to raw catapult data \n",
    "Here we cut session when there is a 1 hour gap between catapult data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(directory):\n",
    "    file_list = os.listdir(directory)\n",
    "\n",
    "    for file in file_list:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path, parse_dates=['hour-minutes-second'], date_parser=lambda x: pd.to_datetime(x, format='%H-%M-%S'))\n",
    "\n",
    "        # Trier par heure\n",
    "        df = df.sort_values(by='hour-minutes-second')\n",
    "\n",
    "        # Créer des sessions basées sur l'écart d'une heure\n",
    "        df['session_id'] = (df['hour-minutes-second'].diff().dt.seconds.fillna(0) > 3600).cumsum() + 1\n",
    "\n",
    "        # Sauvegarder le fichier\n",
    "        output_path = os.path.join(\"data/test/catapult_raw_with_session\", file)\n",
    "        df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_files(\"../data/dataset_9/catapult_data\")\n",
    "process_files(\"../data/dataset_9/test_data/catapult_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef34e33",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29935cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_column(df):\n",
    "    return df.drop(['hour-minutes-second', \"Centiseconds\", \"x\", \"y\", \"Metabolic power\", 'Latitude', 'Longitude'], axis=1)\n",
    "\n",
    "def concat_csv_files(folder_path):\n",
    "    # Liste pour stocker les DataFrames de chaque fichier CSV\n",
    "    dfs = []\n",
    "\n",
    "    # Parcours de tous les fichiers du dossier\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Lecture du fichier CSV et ajout du DataFrame à la liste\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = drop_useless_column(df)\n",
    "            dfs.append(df)\n",
    "\n",
    "    # Concaténation de tous les DataFrames en un seul\n",
    "    result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def fill_missing_heart_rate_values(df):\n",
    "    # Créer une série pour stocker les valeurs remplacées\n",
    "    filled = df[\"Heart rate\"].copy()\n",
    "\n",
    "    # Identifier les cellules à remplir\n",
    "    mask = filled < 2.0\n",
    "\n",
    "    # Trouver les valeurs suivantes et précédentes valides\n",
    "    next_valid = df.groupby('catapult_id')[\"Heart rate\"].apply(lambda x: x.shift(-1).ffill())\n",
    "    prev_valid = df.groupby('catapult_id')[\"Heart rate\"].apply(lambda x: x.shift(1).bfill())\n",
    "\n",
    "    # Choisir la valeur suivante ou précédente en fonction de la condition\n",
    "    filled[mask] = next_valid[mask].where(next_valid[mask] > 2.0, prev_valid[mask])\n",
    "\n",
    "    # Assurer que les cellules sans valeurs valides soient mises à 0\n",
    "    filled.fillna(0.0, inplace=True)\n",
    "    \n",
    "    df['Heart rate'] = filled\n",
    "\n",
    "def heart_rate(df):\n",
    "    # Grouper par 'catapult_id' et calculer la valeur maximale de 'Heart rate' pour chaque groupe\n",
    "    max_heart_rate = df.groupby('catapult_id')['Heart rate'].max()\n",
    "\n",
    "    # Créer une nouvelle colonne 'Heart_rate_Max' dans le DataFrame initial\n",
    "    df['Heart_rate_Max'] = df['catapult_id'].map(max_heart_rate)\n",
    "    \n",
    "    # Calculer le ratio de 'Heart rate' sur 'Heart_rate_Max' et créer une nouvelle colonne 'Heart_rate_Ratio'\n",
    "    df['Heart_rate_Ratio'] = df['Heart rate'] / df['Heart_rate_Max']\n",
    "    df.drop([\"Heart rate\", \"Heart_rate_Max\"], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def create_player_dfs(player_info_data_path, concatenated_data):\n",
    "    \"\"\"On remplace les catapult_id par player_id correspondants et on crée un dataframe par player, stockés dans un dictionnaire dont les clefs sont les player_id.\"\"\"\n",
    "    player_info_data = pd.read_csv(player_info_data_path)\n",
    "    catapult_id_player = dict(zip(player_info_data['catapult_id'], player_info_data['player_id']))\n",
    "    concatenated_data['catapult_id'] = concatenated_data['catapult_id'].replace(catapult_id_player)\n",
    "\n",
    "    players = concatenated_data.groupby('catapult_id')\n",
    "    return {player_id: player_df.drop([\"catapult_id\"], axis=1) for player_id, player_df in players}\n",
    "\n",
    "def seperate_by_session(players_df, out_path):\n",
    "    \"\"\"Open each player file and create a csv file for each player and each training session (defined by the date and session_id)\"\"\"\n",
    "    file_list = players_df.keys()\n",
    "    \n",
    "    players_df_list = []\n",
    "    for player_id in file_list:\n",
    "        df = players_df[player_id]\n",
    "        \n",
    "        groups_by_session = df.groupby(['n_date', 'session_id'])\n",
    "\n",
    "        for (date, session_id), group in groups_by_session:\n",
    "            \n",
    "\n",
    "            player_folder = os.path.join(out_path, str(player_id))\n",
    "\n",
    "            if not os.path.exists(player_folder):\n",
    "                os.mkdir(player_folder)\n",
    "            \n",
    "            group.to_csv(f'{player_folder}/{date}_session_{session_id}.csv', index=False)\n",
    "\n",
    "def get_max_length_by_player(file_path, out_path):\n",
    "    \"\"\"Getting the max length of training session for each player, convert them to a multiple of 4000, store them in a dictionary, then save it as json\"\"\"\n",
    "\n",
    "    player_files = os.listdir(file_path)\n",
    "\n",
    "    max_length_by_player = {}\n",
    "\n",
    "    for player_file in player_files:\n",
    "        player_file_path = file_path + \"/\" + player_file\n",
    "        session_training_file = os.listdir(player_file_path)\n",
    "        max_length_by_player[player_file] = 0\n",
    "\n",
    "        for session_csv in session_training_file:\n",
    "            df = pd.read_csv(player_file_path + \"/\" + session_csv)\n",
    "            if df.shape[0] > max_length_by_player[player_file]:\n",
    "                max_length_by_player[player_file] = df.shape[0]\n",
    "\n",
    "\n",
    "    for key in max_length_by_player.keys():\n",
    "        max_length_by_player[key] = 4000 * (int(max_length_by_player[key] // 4000) + 1)\n",
    "\n",
    "    with open(out_path, 'w') as fichier_json:\n",
    "        json.dump(max_length_by_player, fichier_json)\n",
    "    \n",
    "    return max_length_by_player\n",
    "\n",
    "def pad_session(session, max_size_session):\n",
    "    nb_zeros = max_size_session - len(session)\n",
    "    zero_rows = pd.DataFrame(0, index=range(nb_zeros), columns=session.columns)\n",
    "    zero_rows[\"n_date\"] = session[\"n_date\"].iloc[0]\n",
    "    zero_rows[\"session_id\"] = session[\"session_id\"].iloc[0]\n",
    "\n",
    "    session = pd.concat([zero_rows, session], ignore_index=True)\n",
    "    return session\n",
    "\n",
    "def pad_and_normalize_all_sessions(file_path, out_path, max_length_by_player):\n",
    "    \"\"\"For each player, pad all session to the max length.\n",
    "    Padding means filling the line with zeros at the beginning of the dataset, \n",
    "    to obtain a dataset of size max_length_by_player, constant for each player. \"\"\"\n",
    "\n",
    "    player_files = os.listdir(file_path)\n",
    "\n",
    "    for player_file in player_files:\n",
    "        player_file_path = file_path + \"/\" + player_file\n",
    "        session_training_file = os.listdir(player_file_path)\n",
    "\n",
    "        session_df_list = []\n",
    "        for half_day_csv in session_training_file:\n",
    "            df = pd.read_csv(player_file_path + \"/\" + half_day_csv)\n",
    "            session_df_list.append(pad_session(df, max_length_by_player[player_file]))\n",
    "\n",
    "        df_concat = pd.concat(session_df_list, axis=0)\n",
    "        \n",
    "        df_concat.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        normalized_df = pd.DataFrame(scaler.fit_transform(df_concat.drop([\"n_date\", \"session_id\"], axis=1)), columns=df_concat.drop([\"n_date\", \"session_id\"], axis=1).columns)\n",
    "\n",
    "        normalized_df[\"n_date\"] = df_concat[\"n_date\"]\n",
    "        normalized_df[\"session_id\"] = df_concat[\"session_id\"]\n",
    "        normalized_df.to_csv(out_path + player_file + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e5a3a1",
   "metadata": {},
   "source": [
    "# Process catapult data\n",
    "\n",
    "We seperate the code in 5 parts to avoid kernel crash. If needed, there are commented lines for checkpoints.\n",
    "1. Training data (~2 hours)\n",
    "2. Test data (~20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94471c52",
   "metadata": {},
   "source": [
    "### 1. Training data (~2 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b50594",
   "metadata": {},
   "source": [
    "1. Concatenating all data per player\n",
    "2. Cleaning Hear rate feature\n",
    "3. Split concatenated dataset by player\n",
    "4. Split each player dataset by session\n",
    "5. Get max session length for each player\n",
    "6. Pad and normalize session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acfc9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"data/catapult_raw_with_session/\"\n",
    "session_path = 'data/catapult_by_player_by_session/'\n",
    "\n",
    "concatenated_data = concat_csv_files(raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "141e0aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_data[\"Heart rate\"].fillna(0)\n",
    "concatenated_data = heart_rate(concatenated_data)\n",
    "#concatenated_data.to_csv(\"data/checkpoints/concatenated_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4adb391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create catapult dataframes for each player, in a dictionary\n",
    "players_df = create_player_dfs(\"../data/dataset_9/player_info.csv\", concatenated_data)\n",
    "#for key in players_df:\n",
    "#    players_df[key].to_csv(f\"data/checkpoints/player_df/{key}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b585cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate and save each session, for each player\n",
    "seperate_by_session(players_df, session_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a75b77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the max length of training session for each player\n",
    "max_length_by_player = get_max_length_by_player(session_path, session_path + 'max_length_by_player.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding and normalizing each session and save final datasets \n",
    "pad_and_normalize_all_sessions(session_path, \"data/catapult_by_player_normalized/\", max_length_by_player)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf311d7c",
   "metadata": {},
   "source": [
    "### 2. Test data (~20 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f289faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"data/test/catapult_raw_with_session/\"\n",
    "session_path = 'data/test/catapult_by_player_by_session/'\n",
    "\n",
    "concatenated_data = concat_csv_files(raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de64c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_data[\"Heart rate\"].fillna(0)\n",
    "concatenated_data = heart_rate(concatenated_data)\n",
    "#concatenated_data.to_csv(\"data/test/checkpoints/concatenated_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1f121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create catapult dataframes for each player, in a dictionary\n",
    "players_df = create_player_dfs(\"../data/dataset_9/player_info.csv\", concatenated_data)\n",
    "#for key in players_df:\n",
    "#    players_df[key].to_csv(f\"data/test/checkpoints/player_df/{key}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb325e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate and save each session, for each player\n",
    "seperate_by_session(players_df, session_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779437ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the max length of training session for each player\n",
    "max_length_by_player = get_max_length_by_player(session_path, session_path + 'max_length_by_player.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3eb41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding and normalizing each session and save final datasets \n",
    "pad_and_normalize_all_sessions(session_path, \"data/test/catapult_by_player_normalized/\", max_length_by_player)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
