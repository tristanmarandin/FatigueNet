{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3a848b",
   "metadata": {},
   "source": [
    "# **PREPROCESS JM (JOUEUR-METEO) DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b804b9b2",
   "metadata": {},
   "source": [
    "- Useful functions\n",
    "- Merging and normalization datasets \n",
    "- Split by player_id\n",
    "- Remove inconsistent date data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3cd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188c89e",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab3ac6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_day(h):\n",
    "    if h >= datetime.strptime(\"12:00\",'%H:%M') and h < datetime.strptime(\"23:00\",'%H:%M') :\n",
    "        return \"PM\"\n",
    "    \n",
    "    elif h >= datetime.strptime(\"08:00\",'%H:%M') and h < datetime.strptime(\"12:00\",'%H:%M'):\n",
    "        return \"AM\"\n",
    "    \n",
    "\n",
    "def preprocess_meteo(meteo_data):\n",
    "\n",
    "    meteo_data['Heure'] = pd.to_datetime(meteo_data['Heure'], format='%H:%M')\n",
    "    \n",
    "    meteo_data['AM/PM'] = meteo_data['Heure'].apply(half_day)\n",
    "    \n",
    "    meteo_data_processed = meteo_data.drop(['Heure', 'is_day ()'], axis=1)\n",
    "    \n",
    "    meteo_data_processed = meteo_data_processed.groupby(['time', 'AM/PM']).mean()\n",
    "    \n",
    "    return meteo_data_processed\n",
    "\n",
    "\n",
    "def merge_datasets(data_rpe, data_meteo, data_anthropo):\n",
    "\n",
    "    # set the right types and indexes\n",
    "    data_rpe['n_date'] = pd.to_datetime(data_rpe[\"n_date\"])\n",
    "    data_meteo.reset_index(inplace=True)\n",
    "    data_meteo['time'] = pd.to_datetime(data_meteo['time'])\n",
    "\n",
    "    # merge rpe and meteo\n",
    "    merged_data=pd.merge(data_rpe,data_meteo, left_on=[\"n_date\",\"am-pm\"], right_on=[\"time\",\"AM/PM\"], how='left')\n",
    "    merged_data.drop([\"time\", \"AM/PM\"], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    merged_data['n_date'] = pd.to_datetime(merged_data['n_date'])\n",
    "    data_anthropo['n_date'] = pd.to_datetime(data_anthropo['n_date'])\n",
    "    data_anthropo = data_anthropo.sort_values(['n_date', 'player_id'])\n",
    "\n",
    "    # merge with anthropo (approximative on date, exact on player_id)\n",
    "    merged_data = pd.merge_asof(merged_data, data_anthropo, on='n_date', by='player_id', direction='forward')\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faebad05",
   "metadata": {},
   "source": [
    "## Merging and normailze datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "726b23d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"../data/dataset_9/\"\n",
    "\n",
    "def merge_and_normalize(is_train=True):\n",
    "    TEST_DATA_DIR = \"\"  \n",
    "    TEST_DIR = \"\"\n",
    "    if not is_train : \n",
    "        TEST_DATA_DIR = \"/test_data\" \n",
    "        TEST_DIR = \"/test\" \n",
    "\n",
    "    data_anthropo = pd.read_csv(DIR + \"data_anthropo.csv\")\n",
    "    meteo_data = pd.read_csv(DIR + \"meteo_data.csv\")\n",
    "    RPE_TRAIN_FINAL = pd.read_csv(DIR + TEST_DATA_DIR + \"/RPE_TEST_FINAL.csv\")\n",
    "\n",
    "    # Process weather data\n",
    "    meteo_processed = preprocess_meteo(meteo_data)\n",
    "\n",
    "    # Merging\n",
    "    merged_df = merge_datasets(data_rpe=RPE_TRAIN_FINAL, data_meteo=meteo_processed, data_anthropo=data_anthropo)\n",
    "    merged_df = merged_df.drop([\"hour\", \"IS_TEST\", 'team'], axis=1)\n",
    "    merged_df = merged_df.replace('#DIV/0!', np.nan)\n",
    "\n",
    "    # Normalization\n",
    "    scaler = StandardScaler()\n",
    "    data_final_copy = merged_df\n",
    "    normalized_df = pd.DataFrame(scaler.fit_transform(merged_df.drop([\"RPE\",'player_id','n_date','am-pm'], axis=1)), columns=merged_df.drop(['RPE','player_id','n_date','am-pm'], axis=1).columns)\n",
    "    normalized_df[\"RPE\"] = merged_df[\"RPE\"]\n",
    "    normalized_df['player_id'] = merged_df['player_id']\n",
    "    normalized_df['n_date'] = merged_df['n_date']\n",
    "    normalized_df['am-pm'] = merged_df['am-pm']\n",
    "\n",
    "    # Save\n",
    "    normalized_df.to_csv(\"data\" + TEST_DATA_DIR + \"/processed_JM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54879fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_and_normalize(is_train=True)\n",
    "merge_and_normalize(is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1266c409",
   "metadata": {},
   "source": [
    "## Split by player ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session_id(df):\n",
    "    df['session_id'] = df.groupby('n_date').cumcount() + 1\n",
    "    df.drop(['am-pm', 'player_id'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "def split_by_player(in_path, out_path):\n",
    "\n",
    "    df = pd.read_csv(in_path).fillna(0)\n",
    "    group_by_player = df.groupby(['player_id'])\n",
    "    jm_by_player_dict = {}\n",
    "\n",
    "    for player_id, jm_data in group_by_player:   \n",
    "        player_id = player_id[0]\n",
    "        create_session_id(jm_data)\n",
    "\n",
    "        jm_by_player_dict[player_id] = jm_data\n",
    "        player_folder = os.path.join(out_path, str(player_id))\n",
    "\n",
    "        if not os.path.exists(player_folder):\n",
    "            os.mkdir(player_folder)\n",
    "        \n",
    "        jm_data.to_csv(f'{player_folder}/{player_id}.csv', index=False)\n",
    "        \n",
    "        print(f'{player_id}: {jm_data.shape[0]}')\n",
    "    return jm_by_player_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jm_by_player_dict = split_by_player(\"data/processed_JM.csv\", \"data/jm_by_player/\")\n",
    "jm_by_player_dict_test = split_by_player(\"data/test/processed_JM.csv\", \"data/test/jm_by_player/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3907edf4",
   "metadata": {},
   "source": [
    "## Remove inconsistent date data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5c16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-12-04', '2020-12-28', '2021-02-02', '2021-02-10', '2021-02-17', '2021-03-03', '2021-03-16', '2021-03-23', '2021-03-31', '2021-04-02', '2021-04-06', '2021-01-05', '2021-02-05', '2021-02-09', '2021-02-25', '2021-04-20', '2021-04-20']\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert the list of file name (2021-02-10_session_1) to dict {\"2021-02-10\": 1}\n",
    "def list_to_dictionary(file_list):\n",
    "    date_dictionary = defaultdict(int)\n",
    "    for file in file_list:\n",
    "        date, session = file.split('_session_')\n",
    "        if session.startswith('1'):\n",
    "            date_dictionary[date] += 1\n",
    "    return dict(date_dictionary)\n",
    "\n",
    "# Here we delete dates that are not in both Catapult and JM data\n",
    "def adjust_dataframe_and_keep_dates(df, date_dictionary):\n",
    "    to_delete_dates = []\n",
    "    for date, expected_count in date_dictionary.items():\n",
    "        actual_count = df[df['n_date'] == date].shape[0]\n",
    "        if actual_count != expected_count:\n",
    "            to_delete_dates.append(date)  # Keep the date in a list\n",
    "    for date in df['n_date'].tolist():\n",
    "        if date not in date_dictionary.keys():\n",
    "            to_delete_dates.append(date)\n",
    "    for date in to_delete_dates:\n",
    "        df = df[df['n_date'] != date]  # Remove all rows corresponding to this date\n",
    "\n",
    "    return df, to_delete_dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58d2ae",
   "metadata": {},
   "source": [
    "### TRAIN REMOVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406e7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_folder = os.listdir(\"data/catapult_by_player_by_session\")\n",
    "\n",
    "for player_id in player_folder:\n",
    "    print(player_id)\n",
    "    session_files = os.listdir(\"data/catapult_by_player_by_session/\" + str(player_id))\n",
    "    try:\n",
    "        df = pd.read_csv(f\"data/jm_by_player/{player_id}/{player_id}.csv\")\n",
    "    except:\n",
    "        print(f\"No player found: {player_id}\")\n",
    "\n",
    "    dictionnaire_dates = list_to_dictionary(session_files)\n",
    "    df_ajuste, to_delete_dates = adjust_dataframe_and_keep_dates(df, dictionnaire_dates)\n",
    "    \n",
    "    catapult = pd.read_csv(\"data/catapult_by_player_normalized/\" + player_id + \".csv\")\n",
    "\n",
    "    catapult = catapult[~catapult['n_date'].isin(to_delete_dates)]\n",
    "    \n",
    "    df_ajuste.to_csv(\"data/jm_clean_by_player/\" + player_id + \".csv\")\n",
    "    catapult.to_csv(\"data/catapult_by_player_final/\" + player_id + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f577f135",
   "metadata": {},
   "source": [
    "### TEST REMOVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a7b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_folder = os.listdir(\"data/test/catapult_by_player_by_session\")\n",
    "\n",
    "for player_id in player_folder:\n",
    "    #if (player_id in already_done or player_id == '27943'):\n",
    "    #    continue\n",
    "    print(player_id)\n",
    "    session_files = os.listdir(\"data/test/catapult_by_player_by_session/\" + str(player_id))\n",
    "    df = pd.read_csv(f\"data/test/jm_by_player/{player_id}/{player_id}.csv\")\n",
    "\n",
    "    dictionnaire_dates = list_to_dictionary(session_files)\n",
    "    df_ajuste, to_delete_dates = adjust_dataframe_and_keep_dates(df, dictionnaire_dates)\n",
    "    print(to_delete_dates)\n",
    "    \n",
    "    catapult = pd.read_csv(\"data/test/catapult_by_player_normalized/\" + player_id + \".csv\")\n",
    "    print(catapult.shape)\n",
    "\n",
    "    catapult = catapult[~catapult['n_date'].isin(to_delete_dates)]\n",
    "    print(catapult.shape)\n",
    "    \n",
    "    df_ajuste.to_csv(\"data/test/jm_clean_by_player/\" + player_id + \".csv\")\n",
    "    catapult.to_csv(\"data/test/catapult_by_player_final/\" + player_id + \".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
